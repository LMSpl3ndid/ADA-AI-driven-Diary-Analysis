import os
import re
import json
from openai import OpenAI
import pandas as pd
from datetime import datetime
import concurrent.futures

# é…ç½®
VAULT_PATH = "2025" # æ—¥è®°æ ¹ç›®å½•
MODEL_NAME = "deepseek-chat"
MODEL_NAME_2 = "deepseek-reasoner"
# å»ºè®®é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼šexport DEEPSEEK_API_KEY="æ‚¨çš„key"
API_KEY = os.environ.get("DEEPSEEK_API_KEY", "your-api-key-here")
BASE_URL = "https://api.deepseek.com"

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

# 1. æ•°æ®æ¸…æ´—å‡½æ•°
def clean_markdown(text):
    text = re.sub(r'^---[\s\S]*?---', '', text)
    text = re.sub(r'```dataview[\s\S]*?```', '', text)
    return text.strip()

# è¾…åŠ©ï¼šä»æ–‡ä»¶åæå–æ ‡å‡†åŒ–æ—¥æœŸ
def extract_date(filename):
    clean_name = filename.replace(" ", "")
    match = re.search(r'(\d{4}-\d{2}-\d{2})', clean_name)
    if match:
        return match.group(1)
    return None

# 2. æ ¸å¿ƒï¼šLLM æå–å™¨ (Map Phase)
def analyze_chunk(date_range, content_chunk):
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæå…¶æ•é”çš„å¿ƒç†å’¨è¯¢å¸ˆå’Œä¼ è®°ä½œå®¶ã€‚è¿™æ˜¯ç”¨æˆ·åœ¨ {date_range} æœŸé—´çš„æ—¥è®°ç‰‡æ®µã€‚
    
    è¯·åˆ†æå¹¶ä»¥çº¯ JSON æ ¼å¼è¾“å‡ºä»¥ä¸‹ä¿¡æ¯ï¼š
    1. "emotion_score": æƒ…ç»ªè¯„åˆ† (-5 åˆ° +5ï¼Œ-5æåº¦ç—›è‹¦ï¼Œ0å¹³é™ï¼Œ+5æåº¦ç‹‚å–œ)ã€‚
    2. "key_events": å‘ç”Ÿçš„å…³é”®äº‹ä»¶åˆ—è¡¨ï¼ˆè¯·éå¸¸å…·ä½“ï¼ŒåŒ…å«é¡¹ç›®åç§°ã€åˆ›ä½œçš„ä½œå“åå¦‚"åˆ¶ä½œæ¯•ä¸šæ­Œ"ã€å…·ä½“åœ°åç­‰ï¼‰ã€‚
    3. "main_focus": ç”¨æˆ·ä¸»è¦èŠ±è´¹ç²¾åŠ›çš„äº‹åŠ¡ï¼ˆå¦‚"å­¦ä¹ Rust"ã€"å‡†å¤‡é©¬æ‹‰æ¾"ï¼‰ã€‚
    4. "highlights": ä»»ä½•å€¼å¾—è®°å½•çš„äººç”Ÿé«˜å…‰æˆ–ä½è°·æ—¶åˆ»ï¼ˆåŒ…æ‹¬æƒ…æ„Ÿæ³¢åŠ¨ã€é‡è¦åæ€ï¼‰ã€‚
    5. "weekly_summary": ä¸€æ®µ100-200å­—çš„æœ¬å‘¨ç”Ÿæ´»æ‘˜è¦ï¼Œä¸²è”å…³é”®äº‹ä»¶ï¼Œæ•æ‰ç”Ÿæ´»ç»†èŠ‚å’Œæ°›å›´ã€‚
    6. "travel_experiences": å…·ä½“çš„æ—…æ¸¸ç»å†ï¼ˆåŒ…æ‹¬åœ°ç‚¹ã€ç‰¹è‰²ä½“éªŒã€å…·ä½“æ„Ÿå—ï¼Œè¯·ä¿ç•™ä¸°å¯Œç»†èŠ‚ï¼Œå°‘æ¦‚æ‹¬ï¼‰ã€‚
    7. "artistic_works": æ¥è§¦çš„æ–‡è‰ºä½œå“ï¼ˆä¹¦ç±ã€ç”µå½±ã€æ¸¸æˆã€éŸ³ä¹ç­‰ï¼Œè¯·åˆ—å‡ºå…·ä½“åç§°å’Œç®€è¦è¯„ä»·/æ„Ÿå—ï¼Œå°‘æ¦‚æ‹¬å¤šç»†èŠ‚ï¼‰ã€‚

    æ—¥è®°å†…å®¹ï¼š
    {content_chunk}
    
    è¯·åªè¾“å‡º JSONï¼Œä¸è¦åŒ…å« Markdown æ ¼å¼æ ‡è®°ã€‚
    """
    
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{'role': 'user', 'content': prompt}],
            response_format={'type': 'json_object'}
        )
        content = response.choices[0].message.content
        print(f"[{date_range}] LLM Response Length: {len(content)}")
        try:
            return json.loads(content)
        except json.JSONDecodeError as e:
            print(f"JSON Decode Error for {date_range}: {e}")
            return None
    except Exception as e:
        print(f"Error processing {date_range}: {e}")
        return None

# 3. ä¸­é—´å±‚ï¼šæœˆåº¦æ€»ç»“å™¨ (Compress Phase)
def generate_monthly_summary(month_str, weekly_data_list):
    month_context = ""
    for item in weekly_data_list:
        month_context += f"""
        ã€æ—¶é—´æ®µ: {item['date_range']}ã€‘
        æ‘˜è¦: {item.get('weekly_summary', '')}
        å…³é”®äº‹ä»¶: {item.get('key_events_str', '')}
        é‡å¿ƒ: {item.get('main_focus_str', '')}
        æ—…æ¸¸: {item.get('travel_experiences_str', '')}
        æ–‡è‰º: {item.get('artistic_works_str', '')}
        -----------------------------------
        """
        
    prompt = f"""
    ä½ æ˜¯ç”¨æˆ·çš„ç”Ÿæ´»ä¼ è®°ä½œè€…ã€‚è¿™æ˜¯ç”¨æˆ·åœ¨ {month_str} æœˆä»½çš„å‡ å‘¨æ—¥è®°åˆ†æç‰‡æ®µã€‚
    è¯·å°†è¿™äº›ç¢ç‰‡ä¿¡æ¯æ•´åˆæˆä¸€ä»½è¿è´¯çš„ã€æœˆåº¦æ€»ç»“ã€‘ã€‚
    
    è¯·ä»¥çº¯ JSON æ ¼å¼è¾“å‡ºï¼š
    1. "month_narrative": æœ¬æœˆå™äº‹ä¸»çº¿ï¼ˆ150-300å­—ï¼‰ï¼Œæ¦‚æ‹¬æœ¬æœˆçš„ç”Ÿæ´»çŠ¶æ€ã€æ ¸å¿ƒå˜åŒ–å’Œå¿ƒè·¯å†ç¨‹ã€‚
    2. "key_achievements": æœ¬æœˆå®Œæˆçš„å…³é”®æˆå°±æˆ–é‡Œç¨‹ç¢‘ï¼ˆåˆ—è¡¨ï¼‰ã€‚
    3. "challenges": æœ¬æœˆé‡åˆ°çš„ä¸»è¦æŒ‘æˆ˜æˆ–ä½è°·ï¼ˆåˆ—è¡¨ï¼‰ã€‚
    4. "month_vibe": æœ¬æœˆçš„æ•´ä½“æ°›å›´/å…³é”®è¯ï¼ˆå¦‚â€œå…µè’é©¬ä¹±â€ã€â€œå®é™è‡´è¿œâ€ï¼‰ã€‚
    5. "travel_art_summary": æœ¬æœˆåœ¨æ—…æ¸¸å’Œæ–‡è‰ºæ–¹é¢çš„äº®ç‚¹æ±‡æ€»ã€‚

    è¾“å…¥æ•°æ®ï¼š
    {month_context}
    
    è¯·åªè¾“å‡º JSONã€‚
    """
    
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME_2,
            messages=[{'role': 'user', 'content': prompt}],
            response_format={'type': 'json_object'}
        )
        content = response.choices[0].message.content
        result = json.loads(content)
        result['month'] = month_str
        
        def safe_join(data):
            if isinstance(data, list):
                return "; ".join([str(item) for item in data])
            return str(data) if data else ""
            
        result['key_achievements_str'] = safe_join(result.get('key_achievements', []))
        result['challenges_str'] = safe_join(result.get('challenges', []))
        
        return result
    except Exception as e:
        print(f"Error generating monthly summary for {month_str}: {e}")
        return None

# 4. å½’æ¡£æ¿å—ç”Ÿæˆå™¨ (Archive Phase)
def generate_archive_section(section_name, raw_data_list, prompt_instruction):
    print(f"æ­£åœ¨æ•´ç†å½’æ¡£æ¿å—: {section_name} ...")
    
    # æ‹¼æ¥åŸå§‹æ•°æ®ï¼Œä¸ºäº†é¿å…è¿‡é•¿ï¼Œå¯ä»¥ç®€å•åŠ ä¸ªæ¢è¡Œ
    full_context = "\n".join([str(item) for item in raw_data_list if item])
    
    # å¦‚æœæ•°æ®é‡è¿‡å¤§ï¼Œå¯èƒ½éœ€è¦æˆªæ–­æˆ–åˆ†æ‰¹ï¼ˆè¿™é‡Œæš‚å‡è®¾deepseek 128kèƒ½holdä½å…¨å¹´çº¯æ–‡æœ¬åˆ—è¡¨ï¼‰
    # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬åªå–å‰100kå­—ç¬¦ï¼ˆçº¦ï¼‰
    if len(full_context) > 100000:
        full_context = full_context[:100000] + "\n...(éƒ¨åˆ†å†…å®¹å› è¿‡é•¿æˆªæ–­)..."

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸ªäººå²æ–™æ•´ç†å‘˜ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ã€å…¨å¹´åŸå§‹è®°å½•æµã€‘ï¼Œæ•´ç†å‡ºä¸€ä»½ç»“æ„æ¸…æ™°ã€ç»†èŠ‚ä¸°å¯Œçš„â€œ{section_name}â€æ¸…å•ã€‚
    
    **æ•´ç†è¦æ±‚ï¼š**
    1. **å»é‡ä¸åˆå¹¶**ï¼šå¯¹äºé‡å¤æåˆ°çš„äº‹ä»¶æˆ–é¡¹ç›®ï¼Œåˆå¹¶ä¸ºä¸€æ¡ï¼Œä¿ç•™æœ€è¯¦ç»†çš„æè¿°ã€‚
    2. **ç»“æ„åŒ–åˆ†ç±»**ï¼š{prompt_instruction}
    3. **ä¿ç•™ç»†èŠ‚**ï¼šä¸è¦åªåˆ—å¤§çº²ï¼Œè¦ä¿ç•™å…·ä½“çš„åœ°åã€ä¹¦åã€äººåã€æƒ…æ„Ÿè¯„ä»·å’Œç‹¬ç‰¹ä½“éªŒã€‚
    4. **æ—¶é—´æ„Ÿ**ï¼šå¦‚æœå¯èƒ½ï¼ŒæŒ‰æ—¶é—´é¡ºåºæˆ–é€»è¾‘é¡ºåºæ’åˆ—ã€‚
    
    è¯·ç›´æ¥è¾“å‡ºæ•´ç†å¥½çš„ Markdown å†…å®¹ï¼Œä¸éœ€è¦å¼€åœºç™½ã€‚
    
    ã€åŸå§‹è®°å½•æµã€‘
    {full_context}
    """
    
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME, # ä½¿ç”¨ chat æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬æ•´ç†
            messages=[{'role': 'user', 'content': prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error generating archive section {section_name}: {e}")
        return f"æ•´ç† {section_name} å¤±è´¥"

# 5. å¹´åº¦æ€»ç»“ç”Ÿæˆå™¨ (Reduce Phase - Dual Track)
def generate_final_summary(monthly_df, output_dir, weekly_df):
    
    # --- Track 1: å™äº‹ç¯‡ (åŸºäºæœˆæŠ¥) ---
    print("æ­£åœ¨ç”Ÿæˆå™äº‹ç¯‡...")
    narrative_context = ""
    for _, row in monthly_df.iterrows():
        narrative_context += f"""
        ã€{row['month']}ã€‘å™äº‹:{row.get('month_narrative','')}| æ°›å›´:{row.get('month_vibe','')}| æˆå°±:{row.get('key_achievements_str','')}| æŒ‘æˆ˜:{row.get('challenges_str','')}
        -------------------
        """
    
    narrative_prompt = f"""
    ä½ æ˜¯ä¸€ä½æ•é”çš„äººç”Ÿå™äº‹è€…ã€‚è¯·åŸºäºä»¥ä¸‹ã€æœˆåº¦å™äº‹æµã€‘ï¼Œæ’°å†™ä¸€ä»½å¹´åº¦æ€»ç»“çš„ **ç¬¬ä¸€éƒ¨åˆ†ï¼šå™äº‹ç¯‡**ã€‚
    
    æ ‡é¢˜ï¼šã€Šæˆ‘çš„2025ï¼š[è¯·æç‚¼å¹´åº¦ä¸»é¢˜è¯]ã€‹
    
    **å†™ä½œè¦æ±‚ï¼š**
    1. **å¹´åº¦å™äº‹å¼§å…‰**ï¼šç”¨â€œå¼€ç¯‡-å‘å±•-é«˜æ½®-æ²‰æ·€â€çš„ç»“æ„ï¼Œè®²è¿°è¿™ä¸€å¹´æˆ‘å¦‚ä½•ä»èµ·ç‚¹å‡ºå‘ï¼Œç»å†æ³¢æŠ˜ï¼Œæœ€ç»ˆè·å¾—æˆé•¿ã€‚
    2. **æ·±åº¦æ´å¯Ÿ**ï¼š
       - **å¹´åº¦é¢å­”**ï¼šä¸ºæˆ‘ç”»ä¸€å¹…è‡ªç”»åƒï¼ˆå‡ ä¸ªå…³é”®èº«ä»½ï¼‰ã€‚
       - **éšç§˜çš„æ—‹å¾‹**ï¼šæŒ‡å‡ºä¸€ä¸ªè´¯ç©¿å…¨å¹´çš„æ·±å±‚è¡Œä¸ºæˆ–æ€ç»´æ¨¡å¼ã€‚
       - **è‡´2026å¹´çš„æˆ‘**ï¼šä¸€å¥æœ‰åŠ›çš„è¯ã€‚
    
    è¯·åªè¾“å‡ºâ€œç¬¬ä¸€éƒ¨åˆ†ï¼šå™äº‹ç¯‡â€çš„å†…å®¹ã€‚
    
    ã€æœˆåº¦å™äº‹æµã€‘
    {narrative_context}
    """
    
    try:
        narrative_response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{'role': 'user', 'content': narrative_prompt}]
        )
        narrative_content = narrative_response.choices[0].message.content
    except Exception as e:
        print(f"ç”Ÿæˆå™äº‹ç¯‡å¤±è´¥: {e}")
        narrative_content = "# å™äº‹ç¯‡ç”Ÿæˆå¤±è´¥"

    # --- Track 2: å½’æ¡£ç¯‡ (åŸºäºå‘¨æŠ¥åŸå§‹æ•°æ®ï¼Œåˆ†å—å¹¶è¡Œ) ---
    print("æ­£åœ¨å¹¶è¡Œç”Ÿæˆå½’æ¡£æ¿å—...")
    
    # å‡†å¤‡æ•°æ®æº
    travel_data = weekly_df['travel_experiences_str'].tolist()
    arts_data = weekly_df['artistic_works_str'].tolist()
    
    # æŠ€æœ¯ä¸åˆ›é€ ï¼šåˆå¹¶ key_events å’Œ main_focus
    tech_data = weekly_df['key_events_str'].tolist() + weekly_df['main_focus_str'].tolist()
    
    # ä¸ªäººæˆé•¿ï¼šåˆå¹¶ summary å’Œ highlights
    growth_data = weekly_df['weekly_summary'].tolist() + weekly_df['highlights'].tolist()
    
    archive_tasks = {
        "è¡Œæ—…ä¸è¶³è¿¹": (travel_data, "è¯·æŒ‰ã€åŸå¸‚/åœ°åŒºã€‘åˆ†ç±»ã€‚åˆ—å‡ºå…·ä½“çš„æ™¯ç‚¹ã€é¤å…ã€ç‹¬ç‰¹ä½“éªŒå’Œå½“æ—¶çš„æ„Ÿå—ã€‚"),
        "ä¹¦å½±éŸ³æ¸¸": (arts_data, "è¯·æŒ‰ã€ä¹¦ç±ã€ç”µå½±ã€åŠ¨ç”»ã€æ¸¸æˆã€éŸ³ä¹ã€‘åˆ†ç±»ã€‚åˆ—å‡ºä½œå“åã€ç®€è¯„å’Œå¸¦æ¥çš„è§¦åŠ¨ã€‚"),
        "æŠ€æœ¯ä¸åˆ›é€ ": (tech_data, "è¯·æŒ‰ã€ç¡¬æ ¸æŠ€æœ¯ç ”ç©¶ã€‘ï¼ˆå¦‚å†…æ ¸ã€AIï¼‰å’Œã€åˆ›é€ æ€§äº§å‡ºã€‘ï¼ˆå¦‚æ¯•ä¸šæ­Œã€è§†é¢‘ï¼‰åˆ†ç±»ã€‚åˆ—å‡ºå…·ä½“é¡¹ç›®ã€æ”»å…‹çš„æŠ€æœ¯éš¾ç‚¹å’Œæˆæœã€‚"),
        "ä¸ªäººæˆé•¿ä¸ç”Ÿæ´»": (growth_data, "è¯·æŒ‰ã€ç”Ÿæ´»é‡Œç¨‹ç¢‘ã€‘ï¼ˆå¦‚å‡å­¦ã€æ¬å®¶ï¼‰ã€ã€æƒ…æ„Ÿä¸åæ€ã€‘ï¼ˆäººé™…ã€å†…è€—ã€å’Œè§£ï¼‰ã€ã€æŠ€èƒ½æ ‘ã€‘ï¼ˆè¿åŠ¨ã€ä¹å™¨ï¼‰åˆ†ç±»ã€‚æ•æ‰å†…å¿ƒçš„å˜åŒ–è½¨è¿¹ã€‚")
    }
    
    archive_contents = {}
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        future_to_section = {
            executor.submit(generate_archive_section, name, data, instr): name 
            for name, (data, instr) in archive_tasks.items()
        }
        
        for future in concurrent.futures.as_completed(future_to_section):
            section_name = future_to_section[future]
            try:
                archive_contents[section_name] = future.result()
                print(f"å®Œæˆå½’æ¡£æ¿å—: {section_name}")
            except Exception as e:
                print(f"å½’æ¡£æ¿å— {section_name} å¼‚å¸¸: {e}")
                archive_contents[section_name] = "ç”Ÿæˆå¤±è´¥"

    # --- æ‹¼æ¥æœ€ç»ˆæ–‡æ¡£ ---
    final_markdown = f"""
{narrative_content}

## ç¬¬äºŒéƒ¨åˆ†ï¼šå½’æ¡£ç¯‡ â€”â€” å²æœˆç•™ç—•çš„æ¸…å•

### 1. ğŸ‘£ è¡Œæ—…ä¸è¶³è¿¹
{archive_contents.get("è¡Œæ—…ä¸è¶³è¿¹", "")}

### 2. ğŸ“š ä¹¦å½±éŸ³æ¸¸
{archive_contents.get("ä¹¦å½±éŸ³æ¸¸", "")}

### 3. ğŸ’» æŠ€æœ¯ä¸åˆ›é€ 
{archive_contents.get("æŠ€æœ¯ä¸åˆ›é€ ", "")}

### 4. ğŸŒ± ä¸ªäººæˆé•¿ä¸ç”Ÿæ´»
{archive_contents.get("ä¸ªäººæˆé•¿ä¸ç”Ÿæ´»", "")}
"""

    print("\n========== å¹´åº¦æ€»ç»“ ==========\n")
    # print(final_markdown) # å†…å®¹å¤ªé•¿ï¼Œä¸å…¨éƒ¨æ‰“å°åˆ°æ§åˆ¶å°
    
    file_path = os.path.join(output_dir, '2025_å¹´åº¦æ€»ç»“_online.md')
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(final_markdown)
    print(f"\nå·²ä¿å­˜è‡³ {file_path}")
    
    # è¯„ä¼°
    evaluate_summary_framework(final_markdown, "åŒè½¨åˆ¶ç”Ÿæˆï¼šæœˆåº¦å™äº‹ + å…¨é‡å‘¨æŠ¥ç»†èŠ‚å½’æ¡£", output_dir)

def evaluate_summary_framework(summary_content, old_prompt, output_dir):
    print("æ­£åœ¨è¯„ä¼°å¹´åº¦æ€»ç»“å¹¶ç”Ÿæˆæ”¹è¿›å»ºè®®...")
    eval_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†™ä½œé¡¾é—®ã€‚è¯·è¯„ä¼°è¿™ç¯‡å¹´åº¦æ€»ç»“ï¼ˆç‰¹åˆ«æ˜¯å½’æ¡£éƒ¨åˆ†çš„ç»†èŠ‚ä¸°å¯Œåº¦ï¼‰ã€‚
    è¯·ç»™å‡ºç®€çŸ­çš„è¯„ä»·å’Œæ”¹è¿›å»ºè®®ï¼Œå¹¶ä¿å­˜ä¸º Markdownã€‚
    """
    try:
        # æˆªå–éƒ¨åˆ†å†…å®¹è¿›è¡Œè¯„ä¼°ï¼Œé¿å…tokenæº¢å‡º
        preview_content = summary_content[:10000] + "\n...(åç•¥)"
        response = client.chat.completions.create(
            model=MODEL_NAME_2,
            messages=[{'role': 'user', 'content': eval_prompt + f"\n\nå†…å®¹é¢„è§ˆï¼š\n{preview_content}"}]
        )
        new_prompt_content = response.choices[0].message.content
        file_path = os.path.join(output_dir, 'New_prompt.md')
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(new_prompt_content)
        print(f"å·²ç”Ÿæˆæ”¹è¿›å»ºè®®ï¼Œä¿å­˜è‡³ {file_path}")
    except Exception as e:
        print(f"è¯„ä¼°åˆ†ææ—¶å‡ºé”™: {e}")

# 6. ä¸»æµç¨‹
def main():
    # åˆ›å»ºè¾“å‡ºç›®å½•
    report_base = "Diary_report"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(report_base, timestamp)
    os.makedirs(output_dir, exist_ok=True)
    print(f"æœ¬æ¬¡è¿è¡Œè¾“å‡ºç›®å½•: {output_dir}")

    results = []
    unique_files = {}

    print("æ­£åœ¨æ‰«æå¹¶å»é‡æ—¥è®°æ–‡ä»¶...")
    if not os.path.exists(VAULT_PATH):
        print(f"Directory {VAULT_PATH} not found.")
        return

    for month_folder in sorted(os.listdir(VAULT_PATH)):
        month_path = os.path.join(VAULT_PATH, month_folder)
        if os.path.isdir(month_path):
            for file_name in os.listdir(month_path):
                if file_name.endswith('.md'):
                    date_str = extract_date(file_name)
                    if date_str:
                        full_path = os.path.join(month_path, file_name)
                        if date_str not in unique_files:
                            unique_files[date_str] = full_path
                        else:
                            if " " not in file_name and " " in os.path.basename(unique_files[date_str]):
                                unique_files[date_str] = full_path
    
    sorted_dates = sorted(unique_files.keys())
    all_files = [{'date': d, 'path': unique_files[d]} for d in sorted_dates]

    print(f"å…±æ‰¾åˆ° {len(all_files)} ç¯‡æœ‰æ•ˆæ—¥è®°ï¼ˆå·²å»é‡ï¼‰ã€‚å¼€å§‹å¤„ç†...")

    chunk_size = 7
    tasks = []

    for i in range(0, len(all_files), chunk_size):
        batch_files = all_files[i:i+chunk_size]
        batch_text = ""
        start_date = batch_files[0]['date']
        end_date = batch_files[-1]['date']
        batch_date_str = f"{start_date} åˆ° {end_date}"
        
        for file_info in batch_files:
            with open(file_info['path'], 'r', encoding='utf-8') as f:
                content = clean_markdown(f.read())
                batch_text += f"ã€æ—¥æœŸ: {file_info['date']}ã€‘\n{content}\n\n"
        
        tasks.append((batch_date_str, batch_text))

    print(f"å…±ç”Ÿæˆ {len(tasks)} ä¸ªå‘¨åˆ†æä»»åŠ¡ï¼Œå‡†å¤‡å¹¶è¡Œå¤„ç† (Max Workers: 10)...")

    # å¹¶è¡Œæ‰§è¡Œå‘¨åˆ†æ
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_date = {executor.submit(analyze_chunk, date_str, text): date_str for date_str, text in tasks}
        
        for future in concurrent.futures.as_completed(future_to_date):
            batch_date_str = future_to_date[future]
            try:
                analysis = future.result()
                print(f"å®Œæˆå‘¨åˆ†æ: {batch_date_str}")
                
                if analysis:
                    analysis['date_range'] = batch_date_str
                    
                    # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨åœ°å°† list æˆ– string è½¬æ¢ä¸ºå¸¦åˆ†éš”ç¬¦çš„å­—ç¬¦ä¸²
                    def safe_join(data):
                        if isinstance(data, list):
                            return "; ".join([str(item) for item in data])
                        return str(data) if data else ""

                    # ä¿å­˜ä¸º _str ä¾›åç»­å¤„ç†å’Œ CSV å¯¼å‡º
                    analysis['key_events_str'] = safe_join(analysis.get('key_events', []))
                    analysis['main_focus_str'] = safe_join(analysis.get('main_focus', []))
                    analysis['travel_experiences_str'] = safe_join(analysis.get('travel_experiences', []))
                    analysis['artistic_works_str'] = safe_join(analysis.get('artistic_works', []))
                    
                    # åŒæ—¶ä¿ç•™åŸå§‹ list/obj æ•°æ®åœ¨å†…å­˜ä¸­ï¼Œä¾› generate_monthly_summary ä½¿ç”¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    # ä½†ä¸ºäº† CSV å¹²å‡€ï¼Œæˆ‘ä»¬åœ¨ç”Ÿæˆ DF å‰æ¸…ç†ä¸€ä¸‹æˆ–åœ¨ DF ç”Ÿæˆåç­›é€‰åˆ—
                        
                    results.append(analysis)
            except Exception as exc:
                print(f"ä»»åŠ¡ {batch_date_str} æŠ›å‡ºå¼‚å¸¸: {exc}")

    results.sort(key=lambda x: x['date_range'])

    if not results:
        print("æœªç”Ÿæˆä»»ä½•åˆ†æç»“æœã€‚")
        return

    # ä¿å­˜å‘¨æŠ¥ CSV (ä»…ä¿ç•™æ¸…æ´—åçš„åˆ—)
    df_weekly = pd.DataFrame(results)
    # å®šä¹‰æœŸæœ›çš„åˆ—é¡ºåºå’Œåç§°
    cols_to_keep = [
        'date_range', 'weekly_summary', 'emotion_score', 
        'key_events_str', 'main_focus_str', 'highlights', 
        'travel_experiences_str', 'artistic_works_str'
    ]
    # ç¡®ä¿åˆ—å­˜åœ¨
    final_cols = [c for c in cols_to_keep if c in df_weekly.columns]
    df_weekly_clean = df_weekly[final_cols]
    
    weekly_csv_path = os.path.join(output_dir, 'diary_analysis_2025_weekly.csv')
    df_weekly_clean.to_csv(weekly_csv_path, index=False, encoding='utf-8-sig')
    print(f"å‘¨æŠ¥æ•°æ®å·²ä¿å­˜è‡³ {weekly_csv_path}")
    
    # æœˆåº¦æ€»ç»“é€»è¾‘
    print("æ­£åœ¨ç”Ÿæˆæœˆåº¦æ€»ç»“...")
    monthly_groups = {}
    for item in results:
        start_date = item['date_range'].split(' ')[0]
        month_key = start_date[:7] 
        if month_key not in monthly_groups:
            monthly_groups[month_key] = []
        monthly_groups[month_key].append(item)
    
    monthly_results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_month = {
            executor.submit(generate_monthly_summary, month, data): month 
            for month, data in monthly_groups.items()
        }
        for future in concurrent.futures.as_completed(future_to_month):
            month = future_to_month[future]
            try:
                m_summary = future.result()
                if m_summary:
                    print(f"å®Œæˆæœˆåº¦æ€»ç»“: {month}")
                    monthly_results.append(m_summary)
            except Exception as e:
                print(f"æœˆåº¦æ€»ç»“ {month} å¤±è´¥: {e}")

    monthly_results.sort(key=lambda x: x['month'])
    
    if monthly_results:
        df_monthly = pd.DataFrame(monthly_results)
        monthly_csv_path = os.path.join(output_dir, 'diary_analysis_2025_monthly.csv')
        # åŒæ ·åªä¿ç•™ str åˆ—ï¼ˆgenerate_monthly_summary å·²å¤„ç†ï¼‰
        df_monthly.to_csv(monthly_csv_path, index=False, encoding='utf-8-sig')
        print(f"æœˆæŠ¥æ•°æ®å·²ä¿å­˜è‡³ {monthly_csv_path}")
        
        # ç”Ÿæˆå¹´åº¦æ€»ç»“ (åŒè½¨åˆ¶ï¼šä¼ å…¥ monthly_df å’Œ weekly_df)
        generate_final_summary(df_monthly, output_dir, df_weekly) # ä¼ å…¥åŸå§‹ weekly_df ä»¥è·å–åˆ—è¡¨æ•°æ®
    else:
        print("æœªç”Ÿæˆæœˆåº¦æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå¹´åº¦æ€»ç»“ã€‚")

if __name__ == "__main__":
    main()